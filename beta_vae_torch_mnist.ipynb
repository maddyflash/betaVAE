{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of beta vae.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP-511P-Ucda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf_iCPzJghL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torchvision\n",
        "import tqdm\n",
        "import sys\n",
        "import uuid\n",
        "import typing\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import IPython\n",
        "\n",
        "from torchvision import transforms as T\n",
        "from torch import nn\n",
        "from IPython import display\n",
        "from IPython.display import Image\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(231)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAovJvzug7Vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "mnist = torchvision.datasets.MNIST(\"./data\",\n",
        "                                   train=True,\n",
        "                                   transform=torchvision.transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "\n",
        "train = torch.utils.data.DataLoader(mnist, batch_size=32,\n",
        "                                    shuffle=True, num_workers=2,\n",
        "                                    pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfe7J4XQXIrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Reshape(nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super(Reshape, self).__init__()\n",
        "        self._shape = tuple(shape)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return x.view(-1, *self._shape)\n",
        "\n",
        "class BVAE(nn.Module):\n",
        "    def __init__(self, latent_dim: int):\n",
        "        super(BVAE, self).__init__()\n",
        "        self._latent_dim = latent_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=64,\n",
        "                      stride=2, kernel_size=3, padding=1),\n",
        "            # 64x13x13                     \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256,\n",
        "                      stride=2, kernel_size=3, padding=1),\n",
        "            # 256x7x7\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=(7 * 7 * 256), out_features=(latent_dim * 2))\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_dim, out_features=(7 * 7 * 128)),\n",
        "            Reshape(128, 7, 7),\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=256,\n",
        "                               kernel_size=3, stride=2,\n",
        "                               padding=1, output_padding=0),\n",
        "            # 256x13x13\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=64,\n",
        "                               kernel_size=3, stride=2,\n",
        "                               padding=0, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            # 64x28x28\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=1,\n",
        "                               kernel_size=3, stride=1,\n",
        "                               padding=1, output_padding=0)\n",
        "            # 1x28x28\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        mean, logvar = self.encode(x)\n",
        "        # why logvar? numerical stability!\n",
        "        assert mean.shape == logvar.shape\n",
        "        z = self.reparam(mean, logvar)\n",
        "        return self.decode(z), mean, logvar\n",
        "\n",
        "    def reparam(self, mean, logvar):\n",
        "        # logvar = log(sigma^2) = 2log(sigma)\n",
        "        # 0.5 * logvar = log(sigma)\n",
        "        # exp(0.5 * logvar) = sigma = std. dev.\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.from_numpy(np.random.normal(0, 1, mean.shape)).to(device, dtype=torch.float32)\n",
        "        return eps * std + mean\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"returns mean and logvar, both of shape N x latent_dim\"\"\"\n",
        "        # self.encoder(x) is of size Nx(latent_dim * 2) where N = batch size\n",
        "        return torch.split(self.encoder(x), self._latent_dim, dim=1)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VREw2mccpeyl",
        "colab_type": "text"
      },
      "source": [
        "The objective function we want to *maximise* is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{z \\sim q}[log P(x|\\mathbf z)] - D_{KL}(q_\\phi(\\mathbf z | x)\\ ||\\ p_\\theta(\\mathbf z))\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "in other words, we want to *minimise*:\n",
        "$$\n",
        "\\begin{align}\n",
        "D_{KL}(q_\\phi(\\mathbf z | x)\\ ||\\ p_\\theta(\\mathbf z)) - \\mathbb{E}_{z \\sim q}[log P(x|\\mathbf z)] \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Turns out, the KL divergence term has a closed form when both $p$ and $q$ are\n",
        "normal distributions. From [Kingma](https://arxiv.org/pdf/1312.6114.pdf):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "-D_{KL}(q_\\phi(\\mathbf z)\\ ||\\ p_\\theta(\\mathbf z)) &= \\int q_\\theta(\\mathbf z)\\ (log\\ p_\\theta(\\mathbf z) - log\\ q_\\theta(\\mathbf z))\\  d\\mathbf z \\\\\n",
        "&= \\frac{1}{2}\\sum_{j=1}^{J}(1 + log\\ \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where both $q$ and $p$ are normal distributions and $J$ is the dimension of the latents (`latent_dim`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUGDj9j7U5bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion(xh, x, mean, logvar, beta):\n",
        "    # mean and logvar have shape [batch_size x latent_dim]\n",
        "\n",
        "    # pxz = -log p(x|z): recall, binary cross entropy = x * -log sig(x') + (1 - x) * -log sig(1 - x')\n",
        "    # reduction=sum => assumed p(x|z) is conditionally independent given z, hence log of a product = sum of logs\n",
        "    pxz = F.binary_cross_entropy_with_logits(xh, x, reduction='sum')\n",
        "    \n",
        "    # compute KL using closed form\n",
        "    kl = -0.5 * (1 + logvar - mean ** 2 - torch.exp(logvar)).sum()\n",
        "    return (pxz + beta * kl) / x.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38e78U11ZjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model: nn.Module, optimiser: torch.optim.Optimizer,\n",
        "                beta: int, epochs: int):\n",
        "    model.train()\n",
        "    tepochs = tqdm.trange(epochs, file=sys.stdout)\n",
        "    losses = []\n",
        "    for e in tepochs:\n",
        "        elosses = []\n",
        "        for x, _ in train:\n",
        "            x = x.to(device)\n",
        "            logits, mean, logvar = model(x)\n",
        "            loss = criterion(logits, x, mean, logvar, beta)\n",
        "            optimiser.zero_grad()\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            elosses.append(loss.item())\n",
        "            tepochs.set_postfix(loss=elosses[-1])\n",
        "        losses.append(np.mean(elosses))\n",
        "    return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGpHYxLTfwqB",
        "colab_type": "text"
      },
      "source": [
        "# Train a $\\beta$-vae model ($\\beta$=10)\n",
        "\n",
        "The reconstructed images seem blurrier than a regular VAE ($\\beta$=1),\n",
        "as the paper reported. The fidelity of these reconstructed images are expected\n",
        "to be higher as $\\beta$ decreases (see below where $\\beta$=1).\n",
        "\n",
        "> I tried $\\beta = 20$ and the reconstructed images are beyond recognititon; not sure if it requires more training epochs or if it's too large for such a simple dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0xS04yncEaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these constants will stay the same for all models\n",
        "LATENT_DIM = 2**4\n",
        "EPOCHS = 10\n",
        "\n",
        "bvae = BVAE(LATENT_DIM).to(device)\n",
        "optimiser = torch.optim.Adam(bvae.parameters())\n",
        "losses = train_model(bvae, optimiser, beta=10, epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNtif4VjgwR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(fig, img: torch.Tensor, title: str=None):\n",
        "    img = np.transpose(img.detach().cpu(), axes=(1, 2, 0)).squeeze()\n",
        "    fig.imshow(img, cmap='gray')\n",
        "    fig.set_xticks(())\n",
        "    fig.set_yticks(())\n",
        "    if title:\n",
        "        fig.set_title(title)\n",
        "    \n",
        "batch1 = next(iter(train))[0].to(device)\n",
        "with torch.no_grad():\n",
        "    x_recon, *_ = bvae(batch1)\n",
        "torch.sigmoid_(x_recon)\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "ax1 = fig.add_subplot(121)\n",
        "imshow(ax1, torchvision.utils.make_grid(x_recon), title=\"Reconstructed\")\n",
        "ax2 = fig.add_subplot(122)\n",
        "imshow(ax2, torchvision.utils.make_grid(batch1), title=\"Original\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFWTMFTshReN",
        "colab_type": "text"
      },
      "source": [
        "# Train a regular VAE model\n",
        "\n",
        "As expected, the fidelity of reconstructed images are higher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_kUyAI_nfye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = BVAE(LATENT_DIM).to(device)\n",
        "optimiser = torch.optim.Adam(vae.parameters())\n",
        "losses = train_model(vae, optimiser, beta=1, epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv4Ty709nibV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    x_recon, *_ = vae(batch1)\n",
        "torch.sigmoid_(x_recon)\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "ax1 = fig.add_subplot(121)\n",
        "imshow(ax1, torchvision.utils.make_grid(x_recon), title=\"Reconstructed\")\n",
        "ax2 = fig.add_subplot(122)\n",
        "imshow(ax2, torchvision.utils.make_grid(batch1), title=\"Original\")\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UiDPSAghOXM",
        "colab_type": "text"
      },
      "source": [
        "# Visualising the latent space\n",
        "\n",
        "`LatentVisualiser` produces GIFs that show how the reconstructed images change over time as the the latent is kept fixed for all but one dimension.\n",
        "Furthermore, the initial latent variable is initilised as a normal vector\n",
        "with mean 0 and variance 1. The following section explores what happens when it's initialised as a 0 vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvE3kT_RTReN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentVisualiser:\n",
        "    def __init__(self, model: nn.Module, dim: int=0,\n",
        "                 init_latent: torch.Tensor=None, width: int=8,\n",
        "                 height: int=8):\n",
        "        assert dim < LATENT_DIM \n",
        "        if init_latent is not None:\n",
        "            self._rnd = init_latent.clone() \n",
        "        else:\n",
        "            self._rnd = torch.from_numpy(np.random.normal(0, 1, rnd_shape)).to(device, dtype=torch.float32)\n",
        "        self._dim = dim\n",
        "        self._filenames = []\n",
        "        self._model = model\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "\n",
        "    def __call__(self, iters: int, step: float, save: str=None):\n",
        "        crnd = self._rnd.clone()\n",
        "        for i in range(1, iters + 1):\n",
        "            crnd[:, self._dim] += step\n",
        "            with torch.no_grad():\n",
        "                recon = torch.sigmoid(self._model.decode(crnd))\n",
        "            fig = plt.figure(figsize=(self._width, self._height))\n",
        "            ax = fig.add_subplot(111)\n",
        "            # hacky!: to prevent similar filenames\n",
        "            filename = f'{uuid.uuid4().hex}.png'\n",
        "            imshow(ax, torchvision.utils.make_grid(recon),\n",
        "                   title=f\"Dimension {self._dim}: {i * step:0.4f}\")\n",
        "            fig.savefig(filename, bbox_inches='tight')\n",
        "            self._filenames.append(filename)\n",
        "            plt.close()\n",
        "        output_filename = save if save else \"_temp.gif\"\n",
        "        assert output_filename.endswith(\".gif\")\n",
        "        self._generate_fig(output_filename)\n",
        "        # with open(output_filename,'rb') as f:\n",
        "        #     display.display(Image(data=f.read(), format='png'))\n",
        "        return output_filename\n",
        "\n",
        "    def _generate_fig(self, output_filename):\n",
        "        assert len(self._filenames)\n",
        "        # taken from https://www.tensorflow.org/tutorials/generative/cvae#generate_a_gif_of_all_the_saved_images\n",
        "        with imageio.get_writer(output_filename, mode='I') as writer:\n",
        "            last = -1\n",
        "            for i, filename in enumerate(self._filenames):\n",
        "                frame = 2*(i**0.5)\n",
        "                if round(frame) > round(last): last = frame\n",
        "                else: continue\n",
        "                image = imageio.imread(filename)\n",
        "                writer.append_data(image)\n",
        "            image = imageio.imread(filename)\n",
        "            writer.append_data(image)\n",
        "    \n",
        "    @staticmethod\n",
        "    def combine_gifs(filenames: typing.List[str], output: str, cols: int) -> str:\n",
        "        # rows = math.ceil(len(filenames) / cols)\n",
        "        assert len(filenames) % cols == 0\n",
        "        rows = int(len(filenames) / cols)\n",
        "        gifs = [imageio.get_reader(f) for f in filenames]\n",
        "        frames = gifs[0].get_length()\n",
        "        assert all(g.get_length() == frames for g in gifs)\n",
        "        gifs = itertools.cycle(gifs)\n",
        "\n",
        "        with imageio.get_writer(output, mode='I') as writer:\n",
        "            for _ in range(frames):\n",
        "                # buf = [[] for _ in range(rows)]\n",
        "                buf = []\n",
        "                for row in range(rows):\n",
        "                    row_buffer = [next(gifs).get_next_data() for _ in range(cols)]\n",
        "                    buf.append(np.hstack(row_buffer))\n",
        "                new_image = np.vstack(buf)\n",
        "                writer.append_data(new_image)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDFvWxwAyQdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of iterations used to increase the latent's value (in the non-fixed dimension)\n",
        "ITERS = 100\n",
        "# step size in each iteration\n",
        "STEP = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chvnbWQch2k2",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the latent space of $\\beta=10$ (normal vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK8hzq0BYu9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnd = torch.from_numpy(np.random.normal(0, 1, size=(32, LATENT_DIM))).to(device, dtype=torch.float32)\n",
        "gifs = []\n",
        "for dim in range(0, LATENT_DIM):\n",
        "    # model with beta=10\n",
        "    gifs.append(LatentVisualiser(bvae, dim=dim, init_latent=rnd)(iters=ITERS, step=STEP, save=f\"beta10_dim{dim}.gif\"))\n",
        "\n",
        "LatentVisualiser.combine_gifs(gifs, \"beta10.gif\", 2)\n",
        "with open(\"beta10.gif\",'rb') as f:\n",
        "    display.display(Image(data=f.read(), format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FG1ZbF9tbkv",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the latent space of $\\beta=1$ (normal vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apd8423bth_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gifs = []\n",
        "for dim in range(0, LATENT_DIM):\n",
        "    # model with beta=1\n",
        "    gifs.append(LatentVisualiser(vae, dim=dim, init_latent=rnd)(iters=ITERS, step=STEP, save=f\"beta1_dim{dim}.gif\"))\n",
        "\n",
        "LatentVisualiser.combine_gifs(gifs, \"beta1.gif\", 2)\n",
        "with open(\"beta1.gif\",'rb') as f:\n",
        "    display.display(Image(data=f.read(), format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3bNO37t03g5",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the latent space of $\\beta=10$ (zero vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpbfHm120Dix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zrs = torch.zeros_like(rnd)\n",
        "gifs = []\n",
        "for dim in range(0, LATENT_DIM):\n",
        "    # model with beta=10\n",
        "    gifs.append(LatentVisualiser(bvae, dim=dim, init_latent=zrs)(iters=ITERS, step=STEP, save=f\"beta10_dim{dim}_zrs.gif\"))\n",
        "\n",
        "LatentVisualiser.combine_gifs(gifs, \"beta10_zrs.gif\", 2)\n",
        "with open(\"beta10_zrs.gif\",'rb') as f:\n",
        "    display.display(Image(data=f.read(), format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPmlIUOr055S",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the latent space of $\\beta=1$ (zero vector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1xE2NRZ0o7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gifs = []\n",
        "for dim in range(0, LATENT_DIM):\n",
        "    # model with beta=1\n",
        "    gifs.append(LatentVisualiser(vae, dim=dim, init_latent=zrs)(iters=ITERS, step=STEP, save=f\"beta1_dim{dim}_zrs.gif\"))\n",
        "\n",
        "LatentVisualiser.combine_gifs(gifs, \"beta1_zrs.gif\", 2)\n",
        "with open(\"beta1_zrs.gif\",'rb') as f:\n",
        "    display.display(Image(data=f.read(), format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0CpXjeeBx0h",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the latent space of $\\beta=10$ (zero vector) with decreasing values\n",
        "\n",
        "Instead of increasing the amount by `STEP`, we decrease it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzzrgOKpBYMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gifs = []\n",
        "for dim in range(0, LATENT_DIM):\n",
        "    # model with beta=10\n",
        "    gifs.append(LatentVisualiser(bvae, dim=dim, init_latent=zrs)(iters=ITERS, step=-STEP, save=f\"beta10_dim{dim}_zrs_n.gif\"))\n",
        "\n",
        "LatentVisualiser.combine_gifs(gifs, \"beta10_zrs_n.gif\", 2)\n",
        "with open(\"beta10_zrs_n.gif\",'rb') as f:\n",
        "    display.display(Image(data=f.read(), format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjvLEYbmCJ8L",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the latent space of $\\beta=1$ (zero vector) with decreasing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfuW-XhlBcJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gifs = []\n",
        "for dim in range(0, LATENT_DIM):\n",
        "    # model with beta=1\n",
        "    gifs.append(LatentVisualiser(vae, dim=dim, init_latent=zrs)(iters=ITERS, step=-STEP, save=f\"beta1_dim{dim}_zrs_n.gif\"))\n",
        "\n",
        "LatentVisualiser.combine_gifs(gifs, \"beta1_zrs_n.gif\", 2)\n",
        "with open(\"beta1_zrs_n.gif\",'rb') as f:\n",
        "    display.display(Image(data=f.read(), format='png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJZGuqiH2jBg",
        "colab_type": "text"
      },
      "source": [
        "# Ideas\n",
        "\n",
        "1. Plot a t-SNE or PCA on the latents to see if similar digits cluster together.\n",
        "\n",
        "2. What's the difference between $\\beta=2$, $\\beta=3$, and so on. Is (say) $\\beta=4$ any different from $\\beta=10$? (inspect visually and probably need to investigate quantitatively using the proposed metric)"
      ]
    }
  ]
}
